<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0069)http://www3.cs.stonybrook.edu/~cvl/content/neuralface/neuralface.html -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks</title>
	<meta name="description" content="Project Page for Extracting semantic Structure from Documents">
    <meta name="author" content="Xiao Yang">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<link href="cvpr2017_doc/bootstrap.min.css" type="text/css" rel="stylesheet">
	<link href="cvpr2017_doc/custom.css" type="text/css" rel="stylesheet">
</head>

<body>

<center><br class="auto-style2">
<table width="1000" style="height: 78px">
	<tbody><tr>
		<td class="auto-style2">&nbsp;</td>
	</tr>
	<tr>
		<td align="center" class="title" colspan="99">
		<h1 class="auto-style4">Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks</h1>
		</td>
	</tr>
</tbody></table>
<br><div class="author">
		<a href="http://personal.psu.edu/xuy111/">Xiao Yang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
		<a href="http://www.meyumer.com/">Ersin Yumer</a><sup>2</sup>&nbsp;&nbsp;&nbsp;
		<a href="https://research.adobe.com/person/paul-asente/">Paul Asente</a><sup>2</sup>&nbsp;&nbsp;&nbsp;
		<a href="https://www.linkedin.com/in/mike-kraley-57925">Mike Kraley</a><sup>3</sup>&nbsp;&nbsp;&nbsp;
		<a href="http://www.cse.psu.edu/~duk17/">Daniel Kifer</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
		<a href="https://clgiles.ist.psu.edu/">C. Lee Giles</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
		</div><br><div class="institute">
		<a href="http://www.psu.edu/">Penn State University</a><sup>1</sup>&nbsp;&nbsp;&nbsp;
		<a href="https://research.adobe.com/">Adobe Research</a><sup>2</sup>&nbsp;&nbsp;&nbsp;
		<a href="https://acrobat.adobe.com/us/en/">Adobe Document Cloud</a><sup>3</sup>&nbsp;&nbsp;&nbsp;
		</div><br><table>
	<tbody><tr>
		
	</tr>
	<tr>
		
	</tr>
</tbody></table>
<table width="800">
	<tbody><tr>
		<td align="center" style="width: 812px"><span class="auto-style3">
		<img src="cvpr2017_doc/examples_synthetic.png" width="800"><br>
		<img src="cvpr2017_doc/examples_real.png" width="800"><br>
		</span></td></tr>
	</tbody>
</table>
<br><table width="800">
	<tbody>
		<tr>
		<td style="width: 812px"><div class="auto-style3">Abstract</div>
		<br>
		<div>We present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that  we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance.</div><br>
		</td></tr>
		<tr>
		<td style="width: 812px"><div class="auto-style3">Network Architecture</div>
		<br>
		<div><img src="cvpr2017_doc/architecture.png" width="800"></div><br>
		</td></tr>
		<tr>
		<td style="width: 812px"><div class="auto-style3">Paper</div>
		<br>
        <div>Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks. Xiao Yang, Ersin Yumer, Paul Asente, Mike, Kraley, Daniel Kifer, C. Lee Giles, IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. (<a href="https://arxiv.org/abs/1706.02337">link</a>)</div><br>
		</td></tr>
		<tr>
        <br>
		<td style="width: 812px"><div class="auto-style3">Supplementary Material</div><br>
		<div><a href="cvpr2017_doc/supplementary.pdf">Supplemental Document</a></div><br>
		<div><a href="cvpr2017_doc/deconv.tar">Code (exclude postprocessing)</a></div><br>
		<div><a href="https://drive.google.com/file/d/1l6JTUG6Ike_KwUhylybmCH1d2ZMwyzZK/view?usp=sharing">DSSE-200 Dataset</a></div><br>
		</td></tr>
		<tr>
		<td style="width: 812px"><div class="auto-style3">Acknowledgement</div><br>
		<div>This work started during Xiao Yang's internship at Adobe Research. This work was supported by NSF grant CCF 1317560 and Adobe Systems Inc. </div><br>
		<br>
		</td>
		<td class="auto-style2">&nbsp;</td>
	    </tr>
</tbody></table>

</center>



</body></html>
